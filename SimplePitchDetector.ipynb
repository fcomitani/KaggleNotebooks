{
  "metadata": {
    "kernelspec": {
      "name": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.5.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "## Introduction ##\n\nHere is a simple script for pitch detection I made in my spare time, it is a bit basic (and not very accurate) but it can be easily expanded and improved.\n\nThere's a lot of literature online explaining different pitch detection algorithms and discussing about their pros and cons for anybody who's interested. You can start [here][1] or [here][2], it's a very fascinating subject in my opinion. \n\nThanks to [Julian][3] for uploading the dataset. As always, comments and suggestions are more than welcome.\n\n\n  [1]: https://en.wikipedia.org/wiki/Pitch_detection_algorithm\n  [2]: https://ccrma.stanford.edu/~pdelac/154/m154paper.htm\n  [3]: https://www.kaggle.com/juliancienfuegos",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Importing libraries and data##",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#Import the libraries needed for this job\nimport sys, os, wave\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nfrom scipy.fftpack import fft\nfrom scipy import signal\nfrom scipy.signal import argrelextrema, fftconvolve",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#Here is a list of notes within the range of a guitar in standard tuning \n#with their frequencies (in Hz)\nnotesList = [['E2', '82.41'], \n        ['F2', '87.31'], \n        ['F#2/Gb2', '92.5'], \n        ['G2', '98'], \n        ['G#2/Ab2', '103.83'], \n        ['A2', '110'], \n        ['A#2/Bb2', '116.54'], \n        ['B2', '123.47'], \n        ['C3', '130.81'], \n        ['C#3/Db3', '138.59'], \n        ['D3', '146.83'], \n        ['D#3/Eb3', '155.56'], \n        ['E3', '164.81'], \n        ['F3', '174.61'], \n        ['F#3/Gb3', '185'], \n        ['G3', '196'], \n        ['G#3/Ab3', '207.65'], \n        ['A3', '220'], \n        ['A#3/Bb3', '233.08'], \n        ['B3', '246.94'], \n        ['C4', '261.63'], \n        ['C#4/Db4', '277.18'], \n        ['D4', '293.66'], \n        ['D#4/Eb4', '311.13'], \n        ['E4', '329.63'], \n        ['F4', '349.23'], \n        ['F#4/Gb4', '369.99'], \n        ['G4', '392'], \n        ['G#4/Ab4', '415.3'], \n        ['A4', '440'], \n        ['A#4/Bb4', '466.16'], \n        ['B4', '493.88'], \n        ['C5', '523.25'], \n        ['C#5/Db5', '554.37'], \n        ['D5', '587.33'], \n        ['D#5/Eb5', '622.25'], \n        ['E5', '659.25']]\n\n#Here we build a dictionary containing all the notes associated to each string\nstrings={'Elo':['E2','E#2,Fb2','F2','F#2/Gb2','G2','G#2/Ab2','A2','A#2/Bb2','B2','C3','C#3/Db3','D3','D#3/Eb3','E3'],\n'A':['A2','A#2/Bb2','B2','C3','C#3/Db3','D3','D#3/Eb3','E3','E#3/Fb3','F3','F#3/Gb3','G3','G#3/Ab3','A3'],\n'D':['D3','D#3/Eb3','E3','E#3/Fb3','F3','F#3/Gb3','G3','G#3/Ab3','A3','A#3/Bb3','B3','C4','C#4/Db4','D4'],\n'G':['G3','G#3/Ab3','A3','A#3/Bb3','B3','C4','C#4/Db4','D4','D#4/Eb4','E4','E#4/Fb4','F4','F#4/Gb4','G4'],\n'B':['B3','C4','C#4/Db4','D4','D#4/Eb4','E4','E#4/Fb4','F4','F#4/Gb4','G4','G#4/Ab4','A4','A#4/Bb4','B4'],\n'Ehi':['E4','E#4/Fb4','F4','F#4/Gb4','G4','G#4/Ab4','A4','A#4/Bb4','B4','C5','C#5/Db5','D5','D#5/Eb5','E5']}\n\n#Convert the list of notes to pandas dataframe\ndf = pd.DataFrame(notesList)\ndf.columns = ['Note', 'Freq']\ndf['Freq']=df['Freq'].astype(float)\n#df.info()\n#df.dtypes\nminFr,maxFr=df['Freq'].min(),df['Freq'].max()\n\n#Load the list of wav files\npath='../input/GuitarNotes/GuitarNotes'\nfiles = [f for f in os.listdir(path) if f.endswith('.wav')]\nfiles.sort()\n\n#We also define a dataframe containing the filenames, we will use this later\ndfa = pd.DataFrame(files)\ndfa.columns = ['File']",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Data format ##\n\nHere we can give a first look at the data provided: each file contains a 16bit audio recording in wav format. We load the first file and plot both the wave and its frequency spectrum by means of the Fourier Transform.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "\n#Load the wav file\nwf = wave.open(path + '/' + files[0])\nsignal = np.fromstring(wf.readframes(-1), 'Int16')\n#Calculate its spectrum with Fast Fourier Transform algorithm\nfreqs = fft(signal)\n\n#Ploth both the wave and its spectrum\nfig = plt.figure(figsize=(8, 6))\ngs1 = gridspec.GridSpec(2, 1, height_ratios=[1, 1]) \nax1, ax2 = fig.add_subplot(gs1[0]), fig.add_subplot(gs1[1])\n\nax1.plot(signal, color=\"#6480e5\")\nax2.plot(np.abs(freqs), color=\"#6480e5\")\n\nax1.set_title('Signal Wave'), ax2.set_title(\"Frequency Plot\")\nax1.set_xlim([0, 1e5]), ax2.set_xlim([0, 5e3])\nax1.set_ylim([-4e4, 4e4]), ax2.set_ylim([0, 1.5e8])\nax1.set_xlabel('Sampling Points'), ax2.set_xlabel('Frequency (Hz)')\nax1.set_ylabel('Amplitude'), ax2.set_ylabel('Intensity') \nax1.ticklabel_format(style='sci', axis='both', scilimits=(0,0)) \nax2.ticklabel_format(style='sci', axis='x', scilimits=(0,0))\n\nplt.tight_layout()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "As expected, the wave composed by a mix of harmonic functions with different frequency, and amplitude. Adding to that, there may be noise and limitations from the recording (it seems for example that the wave amplitude is cut at about 3.2e4, although this doesn't matter to our study). \nThis is confirmed by the representation in the frequencies space, where we can observe a number of more or less broad peaks. Some of them fall well beyond the maximum frequency that a guitar string can reach, suggesting that they are indeed external noise. \n\nAt this point we need a way to selectively identify the adequate frequency to associate with the note pitch. The most naïve choice is just to take the frequency with maximum intensity as it is. This assumption is far from ideal, but it is a good starting point for a simple script.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Identifying the Pitch ##\n\nHere we now define a number of functions that can be used to associate to each file a pitch frequency and subsequently the name of the note.\n\nIn particular, to increase the accuracy of our calculation, the maximum frequency is calculated for smaller windows of the recorded wave and then averaged after taking care of removing the extremes, since it appears that at the algorithm is particularly susceptible to noise at the beginning and at the end of the recording (where the wave carrying the fundamental pitch is too weak).  To further improve the result, one could introduce an interpolation of the identified maximum with neighbour frequencies.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#function to find the fundamental pitch frequency\ndef findFreq(file):\n    chunk = 2048 #increasing the size of these chunks can increase the accuracy but at a computational cost\n    wf = wave.open(path + '/' + file,'r')\n    #we also extract the rate at which the sampling frames were recorded\n    #and the total length of the sample\n    rate = wf.getframerate()\n    swidth = wf.getsampwidth()\n    values=[]\n    timestep = 1.0/rate #the sampling timestep in seconds\n    #now we keep reading the frames contained in each chunk and apply the FFT\n    #until we get to the end of the sample\n    while True:\n        signal = wf.readframes(chunk)\n        if len(signal) < chunk*swidth:\n            break\n        else:\n            signal = np.fromstring(signal, 'Int16')\n            n = signal.size\n            #we apply the FFT, extract the frequencies and take their absolute value\n            freqs = fft(signal)\n            freq = np.fft.fftfreq(n, d=timestep) \n            data = np.abs(freqs)\n            #we find the frequency with maximum intensity \n            max = [x for x in argrelextrema(data, np.greater)[0] if x<10000]\n            maxInt = data[max]\n            absMaxInt=np.max(maxInt)\n            absMax=np.where(maxInt==absMaxInt)[0][0]\n            number=max[absMax]*rate/chunk\n            if number>=minFr and number<=maxFr:\n                values.append(number)\n    #finally, here we consider only the results from the central part of the signal wave\n    #discarding those values that could be affected mostly by noise at the beginnign\n    #and at the end of the recording (the value of one third is completely arbitrary)\n    l=int(len(values)/3)\n    return np.mean(values)\t\n\n#function to extract the note from the name of the file\ndef nameNote(string):\n    for el in [\"C\",\"D\",\"Elo\",\"Ehi\",\"F\",\"G\",\"A\",\"B\"]:\n        if el in string:\n            return el\n    return False\t\n\n#function to identify to which string a note belongs:\n#since some notes can be produced by different strings, we use information on the plucked string \n#(in the name of the file) when available, otherwise we just assume its the string with \n#highest pitch was chosen;\n#this function will be useful to see if the pitch detector functions correctly, since we don't have\n#any other information on which note is found in which file,\n#in the case of the scale notes, where no information on the string is given, a question mark is left\n#although one could try to infer the plucked string somehow\ndef checkString(note, file):\n    notout = ''\n    for el in strings:\n        if note in strings[el]: \n            if el == nameNote(file): return el\n            notout=el\n    return notout\t\t\n\n#here we wrap up everything in a single function, that finds frequency, the closest note \n#to the identified frequency and its string\ndef findNote(file, df):\n\tresults=[]\t\n#\tif nameNote2(file)==False:\treturn False\n\tvalue = findFreq(file)\n\tMinNote=0\n\tMinValue=99999\n\tfor index, row in df.iterrows():\n\t\tif np.abs(value-row['Freq'])<np.abs(MinValue):\n#\t\t\tif '#' not in row['Note'] and 'b' not in row['Note']:\n\t\t\t\tMinValue=value-row['Freq']\n\t\t\t\tMinNote=row['Note']\n\t\t\t\tMinString=checkString(MinNote,file)\n\treturn MinValue, MinNote, MinString",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Applying the Pitch Detector\n========================\n\nNow let's see if the algorithm works on our dataset  and plot for each file the difference between the calculated pitch frequency and the ideal frequency of the closest note.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def analysis(files, algo):\n    v=[]\n    n=[]\n    s=[]\n    for el in files:\n        minValue, minNote, minString = findNote(el,df)\n        v.append(minValue)\n        n.append(minNote)\n        s.append(minString)\n    #we save the results in the analysis dataframe    \n    dfa[algo+'_Diff']=v\n    dfa[algo+'_Not']=n\n    dfa[algo+'_Str']=s\n    #here we join the note to the identified string in a single word    \n    n=[str(a)+'('+str(b)+')' for a,b in zip(n,s)]\t\n    v=np.array(v)\n    \n    \n\n    width=0.75\n    ind = np.arange(len(files))\n    fig = plt.figure(figsize=(8,6))\n    gs2 = gridspec.GridSpec(1, 1) \n    ax = fig.add_subplot(gs2[0])\n\n    #we define two masks for our plot, to distinguish the notes that were identified as sharp or flat\n    mask1 = v > 0.0\n    mask2 = v <= 0.0\n    ax.bar(ind[mask1] + width+0.1, v[mask1], width, color='#6480e5')\n    ax.bar(ind[mask2] + width+0.1, v[mask2], width, color='#e56464')#\n    plt.xticks(ind + width*3/2, files, fontsize=10, rotation='vertical')\n\n    rects = ax.patches\n    labels = [n[i] for i in range(len(n)) if mask1[i]]+[n[i] for i in range(len(n)) if mask2[i]]\n    v2 = [v[i] for i in range(len(v)) if mask1[i]]+[v[i] for i in range(len(v)) if mask2[i]]\n\n    #now make some labels\n    for rect, label, x in zip(rects, labels, v2):\n        height = rect.get_height()\n        if x<0:\n            ax.text(rect.get_x() + rect.get_width()/2, 0.5, label, ha='center', va='bottom', color='black', rotation='vertical')\n        else:\n            ax.text(rect.get_x() + rect.get_width()/2, height + 0.5 , label, ha='center', va='bottom', color='black', rotation='vertical')\n    ax.axhline(y=0, color='k')\n\n    plt.xlim([0,len(v)+rects[-1].get_width()])\n\n\n    ax.set_title('Fundamental Pitch ['+algo+']')\n    ax.set_ylim([-20,20])\n    ax.set_xlim([0,len(v)+1+width/2])\n    ax.set_xlabel('Wav File')\n    ax.set_ylabel('Frequency Difference (Hz)') \n\n    ax.set_aspect(\"auto\")\n    plt.tight_layout()\n    \nanalysis(files, 'FFT')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The results are really interesting. The frequencies identified differ from the ideal values of their closest notes by a few Hz maximum, that should indicate that if the guitar is not much out of tune and the noise and spurious frequencies are not affecting the results too much. If the source of these errors were just the tuning of the guitar, however, we would observe that notes belonging to the same string would be all similarly flat or sharp, that is, their values would be consistently under or over the expected note frequency. In theory one could use this knowledge to further clean the dataset and remove the tuning error.\n\nUnfortunately the algorithm as it is doesn't seem to perform that well as some of the files are associated with strings different from those indicated in their names, but it was able to identify a few notes correctly. However, it can be easily improved, first of all by using more sophisticated methods to identify the frequency (may that be through the autocorrelation function or a more accurate analysis of the spectra).  \n\nNow let's see if using other methods can improve or not the results. We will try here the most basic zero crossing and a more accurate algorithm based on autocorrelation. All we need to do is to redefine the function identifying the frequency and reapply the rest to our dataset.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Zero Crossing ##\n\nThis method is the most naïve, it simply counts the distance between points where the wave crosses the zero axis. It is very simple to understand and implement and fairly inexpensive, but don't expect it to be accurate, especially when the harmonic function is far from ideal, due to noise or multiple sources.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#function to find the fundamental pitch frequency counting zeroes\ndef findFreq(file):\n    wf = wave.open(path + '/' + file,'r')\n    signal = wf.readframes(-1)\n    signal = np.fromstring(signal, 'Int16')\n    #one should be careful in deciding if it is worth analysing the entire record or\n    #just chunks of it, and excluding more noisy parts  \n    #signal=signal[len(signal)/3:len(signal)/2]\n    rate = wf.getframerate()\n    swidth = wf.getsampwidth()\n    #now find the points where the wave crosses the zero axis while rising\n    indices = [i for i in range(1,len(signal)) if signal[i-1]<0 and signal[i]>=0]\n    #again now we could simply take the indices as they are, or we can interpolate them in different\n    #ways to increase the accuracy, here a simple linear interpolation should do the trick\n    cross = [i - (signal[i] / (signal[i+1] - signal[i])) for i in indices]\n    diff=np.diff(cross)\n    #we take only the values that fall within the range of possible guitar notes,\n    #but this doesn't save us from including noise that falls within such range \n    good = [x for x in diff if rate/x>=minFr and rate/x<=maxFr]\n    return rate / np.mean(good)\n\nanalysis(files, 'ZeroCrossing')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "As we can se from the graph, the results are no better than those obtained with the Fourier Transform. They are actually slightly worse. The difference in frequency of a few notes is more then 10Hz from their ideal value. Moreover, comparing the results with the information from the file names, we can see that the algorithm is able to identify most of the strings, with the exception of notes in strings Elo and A, it would be interesting to investigate further the source of this discrepancy, since these are the two lowest strings so it seems to suggest that this algorithm is not ideal for low frequency signals. On the other hand, the notes in the Scale dataset are completely off.\nFrom a few tests, it also seems that this method is extremely sensitive to noise. Changing the range of the analysed signal, for example by taking just smaller chunks of the recorded sound waves, affects heavily the results.  \n\nBefore drawing any conclusion let's proceed with one final algorithm and see what we obtain.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Autocorrelation ##\n\nA more refined method requires to analyse the correlation between the function and a shifter version of itself. If the starting function is harmonic, the resulting autocorrelation function will also be harmonic. The method is the most expensive among the ones here shown, and it can be quite accurate, but its still sensitive to noisy signals and can be thrown off by polyphonic sounds, where multiple pitches are present. ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#function to find the fundamental pitch frequency counting zeroes\ndef findFreq(file):\n    wf = wave.open(path + '/' + file,'r')\n    signal = wf.readframes(-1)\n    signal = np.fromstring(signal, 'Int16')\n    #one should be careful in deciding if it is worth analysing the entire record or\n    #just chunks of it, and excluding more noisy parts  \n    #signal=signal[:len(signal)/2]\n    rate = wf.getframerate()\n    swidth = wf.getsampwidth()\n    #first of all we remove the horizontal offset\n    signal = signal - np.mean(signal)\n    #now we calculate the autocorrelation of the signal against itself but inverted in time\n    #and we throw away negative lags\n    corr = fftconvolve(signal, signal[::-1], mode='full')\n    corr = corr[len(corr)/2:]\n    diff = np.diff(corr)\n    n = [i for i in range(0,len(diff)) if diff[i]>0][0]\n    peak = np.argmax(corr[n:]) + n\n    return rate/peak\n\nanalysis(files, 'Autocorrelation')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The results here are even more interesting, the accuracy of the method is much higher than what observed before, the difference in frequencies is in fact under 5 for all notes with two exceptions. These, (B2.wav and G6.wav) are set to the lowest note in the guitar range and possibly are due to a problem with our algorithm. The frequency ends up being too low, maybe due to noise (but we would observe this effect on more notes) or a bug in the code, which is worth investigating. \n\nIgnoring for a moment theses two outliers, the results for the remaining notes are fairly satisfying.  The respective strings of each note are often indentified correctly, with the exception of B and Ehi, that contain the highest pitch sounds. This is in line with what found in literature, where it is usually observed that autocorrelation is more efficient at mid to low frequencies. In general it seems that the guitar used to record is slightly flat.\n\nLooking at the Scale files, the algorithm correctly recognises the ascending order of the notes, but for some reason there seems to be an offset of two notes lower, Do is seen as an A2 instead of, possibly a C3, Re as a B2 instead of a D3 and so on and so forth. This may be symptom of a systematic error in our algorithm, (maybe in the assignment of the notes). Most likely, the Scale was played on the A string (here some of the notes are labelled as belonging to the D string due to our choice to always choose the string with highest pitch when in doubt) or on both Elo and A, starting from C3 onward.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Comparison ##\n\nNow let's do a quick comparison of the three algorithms exploiting the little information available from the file names.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#We loaded all the ouptut data of the three algorithms in this dataframe, now let's add\n#the information from the file names\ndef nameNoteFile(string):\n    conversion = {\"Do\" : \"C\",\n        \"Re\" : \"D\",\n         \"Mi\" : \"E\",\n        \"Fa\" : \"F\",\n        \"So\" : \"G\",\n        \"La\" : \"A\",\n        \"Ti\" : \"B\"}\n    for el in [\"Do\",\"Re\",\"Mi\",\"Fa\",\"So\",\"La\",\"Ti\"]:\n        if el in string:\n            return conversion[el]\n    return False\n\ndef noteReal(x):\n    if 'Scale' in x: return nameNoteFile(x)\n    else: return ''\n\ndef stringReal(x):\n    if 'Scale' in x: return ''\n    else: return nameNote(x)\n    \ndfa['Real_Not'] = dfa.apply(lambda x: noteReal(x['File']), axis=1)\ndfa['Real_Str'] = dfa.apply(lambda x: stringReal(x['File']), axis=1)\n\n#Now let's have a look at the dataframe\ndfa",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#And now we calculate a few useful quantities\n\n#Here we could check how many notes in the scale have been correctly predicted, but\n#it doesn't tell us much since we have only 5 files\ndfa['FFT_NotCount'] = np.where(dfa['Real_Not']==dfa['FFT_Not'].str[0], np.where('#' in dfa['FFT_Not'], 1, 0), 0)  \ndfa['ZeroCrossing_NotCount'] = np.where(dfa['Real_Not']==dfa['ZeroCrossing_Not'].str[0], 1, 0)\ndfa['Autocorrelation_NotCount'] = np.where(dfa['Real_Not']==dfa['Autocorrelation_Not'].str[0], 1, 0)\n\n#Here we check the number of strings correctly predicted\ndfa['FFT_StrCount'] = np.where(dfa['Real_Str']==dfa['FFT_Str'], 1, 0)\ndfa['ZeroCrossing_StrCount'] = np.where(dfa['Real_Str']==dfa['ZeroCrossing_Str'], 1, 0)\ndfa['Autocorrelation_StrCount'] = np.where(dfa['Real_Str']==dfa['Autocorrelation_Str'], 1, 0)\n\nprint('RESULTS')\nprint('==========================================\\n')\nprint('FFT Scale Notes Prediction: '+'{:.0f}'.format(dfa['FFT_NotCount'][:5].sum()*100/dfa['FFT_NotCount'][:5].count())+'%')\nprint('FFT Strings Notes Prediction: '+'{:.0f}'.format(dfa['FFT_StrCount'][5:].sum()*100/dfa['FFT_StrCount'][5:].count())+'%')\nprint('FFT Mean Frequency Difference: '+'{:.2f}'.format(dfa['FFT_Diff'].abs().mean()) + ' Hz\\n')\nprint('Zero Crossing Scale Notes Prediction: '+'{:.0f}'.format(dfa['ZeroCrossing_NotCount'][:5].sum()*100/dfa['ZeroCrossing_NotCount'][:5].count())+'%')\nprint('Zero Crossing Strings Notes Prediction: '+'{:.0f}'.format(dfa['ZeroCrossing_StrCount'][5:].sum()*100/dfa['ZeroCrossing_StrCount'][5:].count())+'%')\nprint('Zero Crossing Mean Frequency Difference: '+'{:.2f}'.format(dfa['ZeroCrossing_Diff'].abs().mean()) + ' Hz\\n')\nprint('Autocorrelation Scale Notes Prediction: '+'{:.0f}'.format(dfa['Autocorrelation_NotCount'][:5].sum()*100/dfa['Autocorrelation_NotCount'][:5].count())+'%')\nprint('Autocorrelation Strings Notes Prediction: '+'{:.0f}'.format(dfa['Autocorrelation_StrCount'][5:].sum()*100/dfa['Autocorrelation_StrCount'][5:].count())+'%')\nprint('Autocorrelation Mean Frequency Difference: '+'{:.2f}'.format(dfa['Autocorrelation_Diff'].abs().mean()) + ' Hz\\n')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "So, as already mentioned, zero crossing gives the poorest results, with a string note prediction slightly lower than the other two methods (especially in low frequency regions of the spectrum), and a relatively high mean difference between the measured frequencies and the predicted notes.\nOn the other hand, although still unable to predict notes perfectly (but the scale is there!), the algorithm based on autocorrelation gives the best results by far, with a very low mean frequency difference (less then a Hertz) and high string predictions. From our results, this methods is less effective at higher frequencies and seems to be affected by a systematic error in the note recognition snippet that should be investigated.\nFinally, the algorithm based on the Fourier Transform is halfway between the other two, with a string prediction similar to that of the autocorrelation method, but an higher mean frequency difference.\n\nIn this notebook, the three algorithms have been implemented in a straightforward and naïve way, giving us the chance to observe their different properties and efficacy on the available dataset. Refining their implementation and pre-processing of the sound waves files, to either remove or identify possible sources of noise, however, could easily improve the results and give a more accurate pitch detector.",
      "metadata": {}
    }
  ]
}